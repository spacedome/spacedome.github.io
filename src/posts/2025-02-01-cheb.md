---
author: "Spacedome"
desc: "Chebyshev"
keywords: "haskell, blog, chebyshev, algorithms, numerical"
lang: "en"
title: "Functional Chebyshev Approximation"
mathjax: true
---

In a previous post I outlined the Fast Fourier Transform, which has many obvious uses regarding waves and periodic functions.
Personally, I have found more use in non-periodic function approximation, which we do largely with polynomials of various orthogonal bases.
In an incredible twist of fate, it turns out that the most ubiquitous polynomial basis, Chebyshev, is "Fourier in disguise" i.e. we can get one from the other under a special change of variables.
This means in some sense that in his study of the heat equation, Fourier solved not only all questions of periodic approximation, but that one hundred years later it would be realized that he had solved all non-periodic ones as well.
We can then put our DFT to good use solving any problem involving polynomial interpolation on an interval.

# Overview of Chebyshev
I won't belabor the details of approximation theory here. 
For references, I usually turn to Boyd's *Spectral Methods* book (a strange and wonderful gem), or Trefethen's *Approximation Theory and Approximation Practice*, but often the most useful is searching for relevant source code, as the theory is mainly relevant to justification and glosses over implementation.
The one aspect I will discuss, is roughly how we translate from the periodic world of Fourier to the polynomial world of Chebyshev.

The first step of interpolation is choosing what points we wish to interpolate at.
For a periodic domain, we can show in various contexts that uniformly spaced points are optimal for various purposes.
Perhaps surprisingly, this is not true for an interval.
We will take the (non-periodic) interval \\( (-1, 1) \\) as the domain throughout.
There are various methods for choosing a set of points, depending on what constraints you make of the basis functions, and in what sense you wish to optimize.
Here we begin to overlap with the classical study of quadratures, as approximating a function and approximating its integral go hand-in-hand, these are often refered to as quadrature points.
One such set of points are the Chebyshev nodes (there are actually two kinds of Chebyshev polynomials, and hence two sets of points, but we can ignore this here), which are the zeroes of the Chebyshev polynomials.
Looking at the picture of them, we see they correspond exactly to uniformly spaced points on the unit circle, projected onto the real line, giving us our first glimpse into how related this is to Fourier polynomials.
![Chebyshev Nodes](../images/chebyshev-nodes.svg)
From a practical, numerical perspective, we see that this pushes the points out towards the endpoints exponentially, directly counteracting Runge's phenomenon and other issues we encounter when interpolating at uniformly spaced points.

# FFT
Chebyshev polynomials are a critical tool in approximation theory, even for studying existence and convergence of exact (theoretical) solutions.
One might think you could just use Taylor series for everything if you don't care about numerical convergence speed, but this is not the case, and different approximations have vastly different convergence radii and geometry, and can help us understand qualitative aspects of convergence, which can have practical implications.
For the engineer, there is another reason Chebyshev approximation is so valuble, the ability to compute it with the Fast Fourier Transform.
The FFT having linearithmic/log-linear complexity is one of the miracles of signal processing, and we can then leverage seasoned numerical libraries and specialized hardware to get much of Chebyshev for free.
I will not go into how we show this, but the main thrust of it is that Chebyshev can be viewed as Fourier under a somewhat strange looking change of variables \\( T_n(\cos \theta) = \cos(n \theta) \\).
Practically, we can compute the Chebyshev transform by sampling a function at the Chebyshev nodes, mentally thinking of how this projects back onto top half of the unit circle, and we then reflect these points onto the bottom half of the unit circle, giving us twice as many points, which cover \\( [0, 2\pi]\\) in this projection.
We can then apply the FFT (really we only need the Discrete Cosine Transform, half of the FFT) to get the coefficients of our polynomial basis, just like Fourier.
This reflection trick is strangely not often mentioned in the theory, but is critical if you want your implementation to work.

# Approximating a Function
In approximating a function, the question is to what accuracy? 
With Chebyshev, we can expect rapid convergence for anything remotely continuous, much less smooth.
For discontinuous functions, if the discontinuities are known, it is always preferable to approximate in sections, regardless of method, but Chebyshev can often perform in all but the most pathological scenarios, up to geometric (exponential) convergence for analytic functions.
Depending on application, we may be constrained as to the method of sampling, but in general we usually assume computationally cheap sampling across the entire domain, such as in an interactive environment like Mathematica.
With the efficiency of FFT, if there are no hard time constraints (i.e. real-time), we can even adaptively sample until sufficient convergence. 

For the implementer, the main question becomes, in what representation do I store the approximation?
We can go back and forth between storing the samples, and storing the basis coefficients, and both have their advantages.
Using barycentric interpolation on the sample points, we can do quite a lot without using the coefficients at all, with no loss in accuracy.
Using coefficient representation, we can quickly, and with good numerical stability, compute the approximation at a point using the Clenshaw algorithm, a recursive method polynomial evaluation. 
In the sample representation we can perform arithmetic operations, such as adding two functions, trivially (though it may require re-sampling if they have different numbers of samples).
In the coefficient representation, we can differentiate the function by applying differentiation as a linear operator, allowing us to solve differential equations.


# Differentiating a Function & Solving and ODE
It is well known that differentiation is a linear operator, the off-diagonal matrix for differentiating a polynomial in the standard basis is often shown in undergraduate linear algebra courses. 
What is less well known is that when similar ideas are applied to a more numerically useful basis, we can solve huge classes of common ODEs with excellent convergence rates.
This gave rise to an entire field, typically called "Spectral Methods", which solve a differential equation *globally* using some method of collocation, as opposed to Finite Element type methods, which solve *locally* using local basis functions, though these methods are often used in combination.
For Chebyshev methods, we can construct a differential operator directly as a matrix, and even square the matrix to get a second derivative, though there are some tricky questions of conditioning and boundary conditions.
Here is a plot showing a function, it's derivatives, and the Chebyshev approximations.
![Chebyshev Derivatives](../images/chebyshev-deriv.svg)
If one is familiar with ODEs, it is not hard to see how we could use this to solve differential equations, given that we figure out how to represent boundary conditions and initial values.
Here is an example 
![Chebyshev Derivatives](../images/chebyshev-ode.svg)

# Closing Remarks
ApproxFun.jl or Chebfun

# Haskell
